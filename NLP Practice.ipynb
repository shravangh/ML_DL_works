{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7161d03e",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2750547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "80411772",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "61941daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a494eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bf99e9",
   "metadata": {},
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "190930eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f68b59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obj=PorterStemmer()\n",
    "for i in range(len(sentences)):\n",
    "    stem_list=[]\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            stem_word = new_obj.stem(word)\n",
    "            stem_list.append(stem_word)\n",
    "    sentences[i]=' '.join(stem_list) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c2ce536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just\n",
    "\n",
    "# words = [new_obj.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "# words=' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88815c3c",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2552a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a6e4e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_obj=WordNetLemmatizer()\n",
    "for i in range(len(sentences)):\n",
    "    stem_list=[]\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            stem_word = new_obj.lemmatize(word)\n",
    "            stem_list.append(stem_word)\n",
    "    sentences[i]=' '.join(stem_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af6b64",
   "metadata": {},
   "source": [
    "# BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c98d25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3f480517",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d782a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_obj=CountVectorizer(max_features=1500)\n",
    "X_bow=count_obj.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dfcf3673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc989ed7",
   "metadata": {},
   "source": [
    "# TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "079da1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_obj=TfidfVectorizer()\n",
    "X_tfidf=tfidf_obj.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c437e48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 113)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55196a6",
   "metadata": {},
   "source": [
    "# Spam detection in sms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233641b1",
   "metadata": {},
   "source": [
    "# a.using lemmatizer and bag of words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8e19986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "table=pd.read_csv(r'SMSSpamCollection',sep='\\t',names=[\"label\", \"message\"])\n",
    "\n",
    "# data cleaning and preprocessing\n",
    "\n",
    "corpus=[]\n",
    "lemm_obj=WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(table)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', table['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    lem_rev = [lemm_obj.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    lem_rev = ' '.join(lem_rev)\n",
    "    corpus.append(lem_rev)\n",
    "    \n",
    "# bag of words representation\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sms_bow=CountVectorizer(max_features=5000)\n",
    "X_bow=sms_bow.fit_transform(corpus).toarray()\n",
    "\n",
    "y=pd.get_dummies(table['label'])\n",
    "y=y.iloc[:,1].values\n",
    "\n",
    "#splitting the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr , X_te , y_tr , y_te = train_test_split(X_bow,y,test_size = 0.20, random_state = 0)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model=MultinomialNB()\n",
    "model.fit(X_tr,y_tr)\n",
    "\n",
    "y_pred=model.predict(X_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e0141cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[944  11]\n",
      " [  9 151]]\n",
      "0.9820627802690582\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "con=confusion_matrix(y_te, y_pred)\n",
    "print(con)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_te, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d749eb89",
   "metadata": {},
   "source": [
    "# b. lemmatizer and tfidf representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "95867e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning and preprocessing\n",
    "\n",
    "corpus=[]\n",
    "lemm_obj=WordNetLemmatizer()\n",
    "\n",
    "for i in range(len(table)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', table['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    lem_rev = [lemm_obj.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    lem_rev = ' '.join(lem_rev)\n",
    "    corpus.append(lem_rev)\n",
    "    \n",
    "# tfidf vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "sms_tfidf=TfidfVectorizer(max_features=5000)\n",
    "X_tfidf=sms_tfidf.fit_transform(corpus).toarray()\n",
    "\n",
    "y=pd.get_dummies(table['label'])\n",
    "y=y.iloc[:,1].values\n",
    "\n",
    "#splitting the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr , X_te , y_tr , y_te = train_test_split(X_tfidf,y,test_size = 0.20, random_state = 0)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model=MultinomialNB()\n",
    "model.fit(X_tr,y_tr)\n",
    "\n",
    "y_pred=model.predict(X_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6ca6be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[955   0]\n",
      " [ 26 134]]\n",
      "0.9766816143497757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "con=confusion_matrix(y_te, y_pred)\n",
    "print(con)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_te, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2d45d",
   "metadata": {},
   "source": [
    "# c.using stemming and BOW representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e9ff35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning and preprocessing\n",
    "\n",
    "corpus=[]\n",
    "stem_obj=PorterStemmer()\n",
    "\n",
    "for i in range(len(table)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', table['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    stem_rev = [stem_obj.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    stem_rev = ' '.join(stem_rev)\n",
    "    corpus.append(stem_rev)\n",
    "    \n",
    "# bag of words representation\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "sms_bow=CountVectorizer(max_features=5000)\n",
    "X_bow=sms_bow.fit_transform(corpus).toarray()\n",
    "\n",
    "y=pd.get_dummies(table['label'])\n",
    "y=y.iloc[:,1].values\n",
    "\n",
    "#splitting the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr , X_te , y_tr , y_te = train_test_split(X_bow,y,test_size = 0.20, random_state = 0)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model=MultinomialNB()\n",
    "model.fit(X_tr,y_tr)\n",
    "\n",
    "y_pred=model.predict(X_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dc83c94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[946   9]\n",
      " [  8 152]]\n",
      "0.9847533632286996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "con=confusion_matrix(y_te, y_pred)\n",
    "print(con)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_te, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2463170",
   "metadata": {},
   "source": [
    "# d.using stemming and tfidf representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fc000271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning and preprocessing\n",
    "\n",
    "corpus=[]\n",
    "stem_obj=PorterStemmer()\n",
    "\n",
    "for i in range(len(table)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', table['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    stem_rev = [stem_obj.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    stem_rev = ' '.join(stem_rev)\n",
    "    corpus.append(stem_rev)\n",
    "    \n",
    "# tfidf vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "sms_tfidf=TfidfVectorizer(max_features=5000)\n",
    "X_tfidf=sms_tfidf.fit_transform(corpus).toarray()\n",
    "\n",
    "y=pd.get_dummies(table['label'])\n",
    "y=y.iloc[:,1].values\n",
    "\n",
    "#splitting the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr , X_te , y_tr , y_te = train_test_split(X_tfidf,y,test_size = 0.20, random_state = 0)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model=MultinomialNB()\n",
    "model.fit(X_tr,y_tr)\n",
    "\n",
    "y_pred=model.predict(X_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a22f6ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[955   0]\n",
      " [ 29 131]]\n",
      "0.9739910313901345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "con=confusion_matrix(y_te, y_pred)\n",
    "print(con)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_te, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8458de0",
   "metadata": {},
   "source": [
    "# WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "24c5f735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHRAVAN G H\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "90115684",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
    "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
    "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
    "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
    "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
    "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
    "               I have a third vision. India must stand up to the world. Because I believe that unless India \n",
    "               stands up to the world, no one will respect us. Only strength respects strength. We must be \n",
    "               strong not only as a military power but also as an economic power. Both must go hand-in-hand. \n",
    "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of \n",
    "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
    "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life. \n",
    "               I see four milestones in my career\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing the data\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "\n",
    "# Preparing the dataset\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ce8f9580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['.', ',', 'india', 'vision', 'must', 'nation', 'world', 'us', 'three', 'freedom', 'respect', 'see', 'first', 'power', 'yet', '’', 'strength', 'worked', '?', 'life', 'believe', 'dr.', 'great', 'minds', 'one', 'years', 'history', 'developed', 'conquered', 'protect', 'others.that', 'war', 'nurture', 'independence', 'build', 'free', 'tried', 'got', 'started', 'way', 'enforce', 'took', 'culture', 'turks', 'visions', 'people', 'come', 'invaded', 'captured', 'lands', 'alexander', 'onwards', 'greeks', 'moguls', 'land', 'portuguese', 'british', 'french', 'dutch', 'came', 'looted', 'done', 'anyone', 'grabbed', 'second', 'career', 'development', 'respects', 'military', 'also', 'economic', 'go', 'hand-in-hand', 'good', 'fortune', 'vikram', 'sarabhai', 'dept', 'space', 'professor', 'satish', 'dhawan', 'succeeded', 'brahm', 'prakash', 'father', 'nuclear', 'material', 'lucky', 'closely', 'consider', 'opportunity', 'four', 'strong', 'stands', 'fifty', 'unless', 'developing', 'milestones', 'among', 'top', 'nations', 'terms', 'gdp', 'percent', 'growth', 'rate', 'areas', 'poverty', 'levels', 'falling', 'achievements', 'globally', 'recognised', 'today', 'lack', 'self-confidence', 'self-reliant', 'self-assured', 'incorrect', 'third', 'stand', 'time'])\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "words = model.wv.key_to_index.keys()\n",
    "print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8641d96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00219905, -0.00970885,  0.00929075,  0.00203636, -0.00116388,\n",
       "       -0.00551955, -0.0085126 , -0.00989383,  0.00894091, -0.00250522,\n",
       "        0.00459427, -0.00452481,  0.00995189,  0.00366171,  0.00103129,\n",
       "       -0.00403834,  0.00122027, -0.00265451,  0.00735284,  0.00447542,\n",
       "        0.00099955,  0.0034782 ,  0.00372712, -0.00680036,  0.00893242,\n",
       "        0.00173499, -0.00579935,  0.00866838, -0.00129286,  0.00818304,\n",
       "       -0.0014927 ,  0.00698649,  0.00273452, -0.00436226, -0.00374683,\n",
       "        0.00919046,  0.00159645, -0.00599784,  0.00034776, -0.00195135,\n",
       "        0.00159242, -0.00771525,  0.00738298,  0.00131083,  0.00787099,\n",
       "        0.00445568, -0.00439675,  0.00376054, -0.0006357 , -0.00984484,\n",
       "        0.00825004,  0.00964326,  0.00965426, -0.00379659, -0.00844202,\n",
       "        0.00483581, -0.00765107,  0.00853567,  0.00275977,  0.00560496,\n",
       "        0.00611362,  0.00046455, -0.00209463,  0.000778  ,  0.00983559,\n",
       "       -0.00711718, -0.00155744, -0.00235984,  0.00487084,  0.00645515,\n",
       "       -0.0041403 ,  0.00361816, -0.00447258,  0.00326611,  0.0081738 ,\n",
       "        0.00361967, -0.0045711 , -0.00301938,  0.00787179,  0.00959686,\n",
       "        0.00580789, -0.00326881, -0.00183876, -0.00624998, -0.00429521,\n",
       "        0.00336554, -0.00648911, -0.00661903,  0.00811393,  0.00950739,\n",
       "        0.00814451,  0.00150699, -0.00880125, -0.00759764,  0.0015789 ,\n",
       "       -0.00952675, -0.00741688,  0.00203283, -0.00292885, -0.00916266],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding Word Vectors\n",
    "vector = model.wv['war']\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bcd3e9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('consider', 0.24767707288265228),\n",
       " ('looted', 0.217461958527565),\n",
       " ('’', 0.17328032851219177),\n",
       " ('invaded', 0.16238373517990112),\n",
       " ('war', 0.15752418339252472),\n",
       " ('among', 0.1556834876537323),\n",
       " ('opportunity', 0.15104511380195618),\n",
       " ('respects', 0.147064670920372),\n",
       " ('moguls', 0.13638530671596527),\n",
       " ('brahm', 0.13400928676128387)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar=model.wv.most_similar('professor')\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87dd3e",
   "metadata": {},
   "source": [
    "# Word Embedding Techniques using Embedding Layer in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d049c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f92b4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sentences\n",
    "sent=[  'the glass of milk',\n",
    "     'the glass of juice',\n",
    "     'the cup of tea',\n",
    "    'I am a good boy',\n",
    "     'I am a good developer',\n",
    "     'understand the meaning of words',\n",
    "     'your videos are good',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9747dc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the glass of milk',\n",
       " 'the glass of juice',\n",
       " 'the cup of tea',\n",
       " 'I am a good boy',\n",
       " 'I am a good developer',\n",
       " 'understand the meaning of words',\n",
       " 'your videos are good']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9139a708",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vocabulary size\n",
    "voc_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0ad969b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4260, 2529, 3285, 1171], [4260, 2529, 3285, 703], [4260, 3547, 3285, 536], [1826, 3616, 3502, 3719, 721], [1826, 3616, 3502, 3719, 4302], [2843, 4260, 2893, 3285, 945], [912, 2579, 460, 3719]]\n"
     ]
    }
   ],
   "source": [
    "word_len=5000\n",
    "one_hot_list=[]\n",
    "for sentence in sent:\n",
    "    one_hot_list.append(one_hot(sentence,n=word_len))\n",
    "print(one_hot_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "056ea5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0 4260 2529 3285 1171]\n",
      " [   0    0    0    0    0    0 4260 2529 3285  703]\n",
      " [   0    0    0    0    0    0 4260 3547 3285  536]\n",
      " [   0    0    0    0    0 1826 3616 3502 3719  721]\n",
      " [   0    0    0    0    0 1826 3616 3502 3719 4302]\n",
      " [   0    0    0    0    0 2843 4260 2893 3285  945]\n",
      " [   0    0    0    0    0    0  912 2579  460 3719]]\n"
     ]
    }
   ],
   "source": [
    "pad_len=10\n",
    "padded_sentences=pad_sequences(one_hot_list,padding='pre',maxlen=pad_len)\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "480e3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(input_dim=word_len, output_dim=32, input_length=pad_len))\n",
    "model.compile(optimizer='Adam', loss='MeanSquaredError')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1af146a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 10, 32)            160000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,000\n",
      "Trainable params: 160,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4d2f294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  ...\n",
      "  [ 0.04406584 -0.03303982 -0.02856502 ...  0.01554526 -0.03949798\n",
      "   -0.02482674]\n",
      "  [ 0.00212437 -0.00632347 -0.01938267 ...  0.00989449  0.01241541\n",
      "    0.010529  ]\n",
      "  [-0.04738373 -0.03941305  0.03630864 ...  0.00076034  0.03183391\n",
      "   -0.01803321]]\n",
      "\n",
      " [[ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  ...\n",
      "  [ 0.04406584 -0.03303982 -0.02856502 ...  0.01554526 -0.03949798\n",
      "   -0.02482674]\n",
      "  [ 0.00212437 -0.00632347 -0.01938267 ...  0.00989449  0.01241541\n",
      "    0.010529  ]\n",
      "  [-0.02059616  0.00935761 -0.02212101 ... -0.02731514 -0.01669952\n",
      "   -0.03029002]]\n",
      "\n",
      " [[ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  ...\n",
      "  [ 0.03133391 -0.0120767   0.02772577 ... -0.00452847 -0.0381496\n",
      "    0.0304062 ]\n",
      "  [ 0.00212437 -0.00632347 -0.01938267 ...  0.00989449  0.01241541\n",
      "    0.010529  ]\n",
      "  [ 0.02594763  0.03868786  0.04497664 ...  0.04914078  0.02583548\n",
      "   -0.03954942]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  ...\n",
      "  [ 0.02252482  0.04149849 -0.03575335 ...  0.0009824  -0.01604747\n",
      "   -0.00205172]\n",
      "  [-0.01404896 -0.01027924  0.02483641 ...  0.01105607 -0.00464945\n",
      "    0.03951761]\n",
      "  [-0.04428226 -0.00560139  0.04103542 ... -0.03978876 -0.00358564\n",
      "    0.00998081]]\n",
      "\n",
      " [[ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  ...\n",
      "  [ 0.00021806 -0.03016916  0.03595415 ...  0.03628106  0.04993968\n",
      "   -0.02173855]\n",
      "  [ 0.00212437 -0.00632347 -0.01938267 ...  0.00989449  0.01241541\n",
      "    0.010529  ]\n",
      "  [-0.02506077  0.02523193 -0.02968483 ... -0.03558375  0.02553302\n",
      "   -0.03700588]]\n",
      "\n",
      " [[ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  [ 0.02154745  0.02888334  0.01670618 ... -0.04185892 -0.01999984\n",
      "    0.00729573]\n",
      "  ...\n",
      "  [-0.04606193  0.01753093 -0.01152438 ...  0.03699329  0.00437276\n",
      "   -0.03280648]\n",
      "  [ 0.01019983  0.03445861  0.0069866  ... -0.02412409  0.00879803\n",
      "    0.01459695]\n",
      "  [-0.01404896 -0.01027924  0.02483641 ...  0.01105607 -0.00464945\n",
      "    0.03951761]]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(padded_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "754e43ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0, 4260, 2529, 3285, 1171])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36430668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02154745  0.02888334  0.01670618 -0.03140924 -0.02493899 -0.04150356\n",
      "   0.00168527  0.02292116  0.02051813  0.01849495 -0.00530056  0.04585899\n",
      "   0.02039954  0.02585718 -0.00879302  0.02212801 -0.00710664 -0.03175303\n",
      "   0.04641801 -0.04396261  0.02155112  0.00118432  0.04666677 -0.02996508\n",
      "  -0.03453206 -0.01944637  0.02987108  0.03403791  0.04143305 -0.04185892\n",
      "  -0.01999984  0.00729573]\n",
      " [ 0.02154745  0.02888334  0.01670618 -0.03140924 -0.02493899 -0.04150356\n",
      "   0.00168527  0.02292116  0.02051813  0.01849495 -0.00530056  0.04585899\n",
      "   0.02039954  0.02585718 -0.00879302  0.02212801 -0.00710664 -0.03175303\n",
      "   0.04641801 -0.04396261  0.02155112  0.00118432  0.04666677 -0.02996508\n",
      "  -0.03453206 -0.01944637  0.02987108  0.03403791  0.04143305 -0.04185892\n",
      "  -0.01999984  0.00729573]\n",
      " [ 0.02154745  0.02888334  0.01670618 -0.03140924 -0.02493899 -0.04150356\n",
      "   0.00168527  0.02292116  0.02051813  0.01849495 -0.00530056  0.04585899\n",
      "   0.02039954  0.02585718 -0.00879302  0.02212801 -0.00710664 -0.03175303\n",
      "   0.04641801 -0.04396261  0.02155112  0.00118432  0.04666677 -0.02996508\n",
      "  -0.03453206 -0.01944637  0.02987108  0.03403791  0.04143305 -0.04185892\n",
      "  -0.01999984  0.00729573]\n",
      " [ 0.02154745  0.02888334  0.01670618 -0.03140924 -0.02493899 -0.04150356\n",
      "   0.00168527  0.02292116  0.02051813  0.01849495 -0.00530056  0.04585899\n",
      "   0.02039954  0.02585718 -0.00879302  0.02212801 -0.00710664 -0.03175303\n",
      "   0.04641801 -0.04396261  0.02155112  0.00118432  0.04666677 -0.02996508\n",
      "  -0.03453206 -0.01944637  0.02987108  0.03403791  0.04143305 -0.04185892\n",
      "  -0.01999984  0.00729573]\n",
      " [ 0.02154745  0.02888334  0.01670618 -0.03140924 -0.02493899 -0.04150356\n",
      "   0.00168527  0.02292116  0.02051813  0.01849495 -0.00530056  0.04585899\n",
      "   0.02039954  0.02585718 -0.00879302  0.02212801 -0.00710664 -0.03175303\n",
      "   0.04641801 -0.04396261  0.02155112  0.00118432  0.04666677 -0.02996508\n",
      "  -0.03453206 -0.01944637  0.02987108  0.03403791  0.04143305 -0.04185892\n",
      "  -0.01999984  0.00729573]\n",
      " [ 0.02154745  0.02888334  0.01670618 -0.03140924 -0.02493899 -0.04150356\n",
      "   0.00168527  0.02292116  0.02051813  0.01849495 -0.00530056  0.04585899\n",
      "   0.02039954  0.02585718 -0.00879302  0.02212801 -0.00710664 -0.03175303\n",
      "   0.04641801 -0.04396261  0.02155112  0.00118432  0.04666677 -0.02996508\n",
      "  -0.03453206 -0.01944637  0.02987108  0.03403791  0.04143305 -0.04185892\n",
      "  -0.01999984  0.00729573]\n",
      " [-0.02122064 -0.04317107  0.00273838 -0.04694954  0.02482193  0.00795902\n",
      "  -0.02015316 -0.0017462   0.02514914 -0.04434931  0.04575762 -0.03852233\n",
      "  -0.02048028 -0.00550493  0.0060376   0.04451536 -0.00273632  0.01252458\n",
      "  -0.00809759 -0.04178135  0.0342011   0.01437298 -0.02177527  0.03189428\n",
      "  -0.00811718 -0.01508556  0.01195202 -0.02943914 -0.00616424  0.01206617\n",
      "   0.01864981 -0.00357639]\n",
      " [ 0.04406584 -0.03303982 -0.02856502 -0.04034833  0.04609627 -0.01100595\n",
      "  -0.03694735  0.00258013 -0.01878985 -0.00268116 -0.01638206 -0.01083034\n",
      "  -0.03262024  0.01470984 -0.04545372 -0.03917526  0.03553759  0.03935579\n",
      "  -0.02536382 -0.01884118  0.01746206 -0.03830919 -0.00648531  0.03811837\n",
      "   0.02333787  0.04963982  0.04271349  0.03704907 -0.00256579  0.01554526\n",
      "  -0.03949798 -0.02482674]\n",
      " [ 0.00212437 -0.00632347 -0.01938267 -0.02056352  0.009671    0.0389556\n",
      "  -0.00737978 -0.04223796  0.01218962 -0.02047778 -0.01255316 -0.03437786\n",
      "   0.04848646 -0.01585517  0.02658552  0.01692735 -0.00985596  0.00562944\n",
      "   0.01344682  0.04362528  0.00286468 -0.04577807  0.03090659 -0.01390287\n",
      "  -0.01513225  0.00754429  0.02069004 -0.03481262  0.01694781  0.00989449\n",
      "   0.01241541  0.010529  ]\n",
      " [-0.04738373 -0.03941305  0.03630864  0.02073244 -0.04161508  0.01731447\n",
      "  -0.00507513 -0.0282871   0.01765977  0.00807286  0.04318047  0.04728278\n",
      "   0.04915864 -0.04933621 -0.00469991  0.01600257  0.01625295 -0.03173695\n",
      "  -0.01557364 -0.02189374 -0.00991563  0.0376181  -0.04709091 -0.02703604\n",
      "   0.0046343   0.01914854 -0.00288652 -0.0487198  -0.0228598   0.00076034\n",
      "   0.03183391 -0.01803321]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(padded_sentences)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae4366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizers\n",
    "\n",
    "# class Adadelta: Optimizer that implements the Adadelta algorithm.\n",
    "\n",
    "# class Adagrad: Optimizer that implements the Adagrad algorithm.\n",
    "\n",
    "# class Adam: Optimizer that implements the Adam algorithm.\n",
    "\n",
    "# class Adamax: Optimizer that implements the Adamax algorithm.\n",
    "\n",
    "# class Ftrl: Optimizer that implements the FTRL algorithm.\n",
    "\n",
    "# class Nadam: Optimizer that implements the NAdam algorithm.\n",
    "\n",
    "# class Optimizer: Base class for Keras optimizers.\n",
    "\n",
    "# class RMSprop: Optimizer that implements the RMSprop algorithm.\n",
    "\n",
    "# class SGD: Gradient descent (with momentum) optimizer.\n",
    "\n",
    "\n",
    "#losses\n",
    "\n",
    "# Classes\n",
    "\n",
    "# class BinaryCrossentropy: Computes the cross-entropy loss between true labels and predicted labels.\n",
    "\n",
    "# class BinaryFocalCrossentropy: Computes the focal cross-entropy loss between true labels and predictions.\n",
    "\n",
    "# class CategoricalCrossentropy: Computes the crossentropy loss between the labels and predictions.\n",
    "\n",
    "# class CategoricalHinge: Computes the categorical hinge loss between y_true and y_pred.\n",
    "\n",
    "# class CosineSimilarity: Computes the cosine similarity between labels and predictions.\n",
    "\n",
    "# class Hinge: Computes the hinge loss between y_true and y_pred.\n",
    "\n",
    "# class Huber: Computes the Huber loss between y_true and y_pred.\n",
    "\n",
    "# class KLDivergence: Computes Kullback-Leibler divergence loss between y_true and y_pred.\n",
    "\n",
    "# class LogCosh: Computes the logarithm of the hyperbolic cosine of the prediction error.\n",
    "\n",
    "# class Loss: Loss base class.\n",
    "\n",
    "# class MeanAbsoluteError: Computes the mean of absolute difference between labels and predictions.\n",
    "\n",
    "# class MeanAbsolutePercentageError: Computes the mean absolute percentage error between y_true and y_pred.\n",
    "\n",
    "# class MeanSquaredError: Computes the mean of squares of errors between labels and predictions.\n",
    "\n",
    "# class MeanSquaredLogarithmicError: Computes the mean squared logarithmic error between y_true and y_pred.\n",
    "\n",
    "# class Poisson: Computes the Poisson loss between y_true and y_pred.\n",
    "\n",
    "# class Reduction: Types of loss reduction.\n",
    "\n",
    "# class SparseCategoricalCrossentropy: Computes the crossentropy loss between the labels and predictions.\n",
    "\n",
    "# class SquaredHinge: Computes the squared hinge loss between y_true and y_pred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a606328",
   "metadata": {},
   "source": [
    "# lstm_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc88aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f03454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad113046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 5s 0us/step\n",
      "17473536/17464789 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e7213bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
      "<class 'list'>\n",
      "189\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])\n",
    "print(type(X_train[1]))\n",
    "print(len(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc3c84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4ba8dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(numpy.max(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8728f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 600)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    1  194 1153  194    2   78  228    5    6\n",
      " 1463 4369    2  134   26    4  715    8  118 1634   14  394   20   13\n",
      "  119  954  189  102    5  207  110 3103   21   14   69  188    8   30\n",
      "   23    7    4  249  126   93    4  114    9 2300 1523    5  647    4\n",
      "  116    9   35    2    4  229    9  340 1322    4  118    9    4  130\n",
      " 4901   19    4 1002    5   89   29  952   46   37    4  455    9   45\n",
      "   43   38 1543 1905  398    4 1649   26    2    5  163   11 3215    2\n",
      "    4 1153    9  194  775    7    2    2  349 2637  148  605    2    2\n",
      "   15  123  125   68    2    2   15  349  165 4362   98    5    4  228\n",
      "    9   43    2 1157   15  299  120    5  120  174   11  220  175  136\n",
      "   50    9 4373  228    2    5    2  656  245 2350    5    4    2  131\n",
      "  152  491   18    2   32    2 1212   14    9    6  371   78   22  625\n",
      "   64 1382    9    8  168  145   23    4 1690   15   16    4 1355    5\n",
      "   28    6   52  154  462   33   89   78  285   16  145   95]\n"
     ]
    }
   ],
   "source": [
    "max_review_length = 600\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940f8837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 600, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4dce00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 451s 1s/step - loss: 0.4995 - accuracy: 0.7567\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 443s 1s/step - loss: 0.3288 - accuracy: 0.8651\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 454s 1s/step - loss: 0.2529 - accuracy: 0.9010\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 464s 1s/step - loss: 0.2275 - accuracy: 0.9113\n",
      "Epoch 5/10\n",
      "193/391 [=============>................] - ETA: 4:11 - loss: 0.1960 - accuracy: 0.9271"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-afd3f80307e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# Final evaluation of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef02b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
